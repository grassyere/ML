# -*- coding: utf-8 -*-
"""Lab5ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sKwb_x1FT9SrTdOJ260OFwcn6LjDszP0
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from typing import Dict, Tuple
from sklearn.datasets import load_diabetes
from operator import itemgetter
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, r2_score, mean_squared_error, mean_absolute_error, accuracy_score, precision_score
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR, LinearSVR
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.linear_model import LinearRegression

import warnings

warnings.filterwarnings('ignore')
plt.style.use('ggplot')
# %matplotlib inline 
sns.set(style="ticks")

# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes
diabetes = load_diabetes()

diabetes.feature_names

df_diabet = pd.DataFrame(diabetes.data,columns=diabetes.feature_names)
df_diabet['target'] = pd.Series(diabetes.target)
df_diabet.head()

#Построим корреляционную матрицу
fig, ax = plt.subplots(figsize=(15,7))
sns.heatmap(df_diabet.corr(method='pearson'), ax=ax, annot=True, fmt='.2f')

df_diabet.isnull().sum()

X = df_diabet.drop("target", axis = 1)
Y = df_diabet["target"]

diabet_X_train, diabet_X_test, diabet_y_train, diabet_y_test = train_test_split(
   X, Y, test_size=0.25, random_state=1)

fig, ax = plt.subplots(figsize=(8,8)) 
sns.scatterplot(ax=ax, x='bmi', y='target', data=df_diabet)

# Аналитическое вычисление коэффициентов регрессии
def analytic_regr_coef(x_array : np.ndarray, 
                       y_array : np.ndarray) -> Tuple[float, float]:
    x_mean = np.mean(x_array)
    y_mean = np.mean(y_array)
    var1 = np.sum([(x-x_mean)**2 for x in x_array])
    cov1 = np.sum([(x-x_mean)*(y-x_mean) for x, y in zip(x_array, y_array)])
    b1 = cov1 / var1
    b0 = y_mean - b1*x_mean
    return b0, b1

x_array = df_diabet['bmi'].values
   y_array = df_diabet['target'].values

b0, b1 = analytic_regr_coef(x_array, y_array)
b0, b1

# Вычисление значений y на основе x для регрессии
def y_regr(x_array : np.ndarray, b0: float, b1: float) -> np.ndarray:
    res = [b1*x+b0 for x in x_array]
    return res

y_array_regr = y_regr(x_array, b0, b1)

plt.plot(x_array, y_array, 'g.')
plt.plot(x_array, y_array_regr, 'b', linewidth=2.0)
plt.show()

# Простейшая реализация градиентного спуска
def gradient_descent(x_array : np.ndarray,
                     y_array : np.ndarray,
                     b0_0 : float,
                     b1_0 : float,
                     epochs : int,
                     learning_rate : float = 0.001
                    ) -> Tuple[float, float]:
    # Значения для коэффициентов по умолчанию
    b0, b1 = b0_0, b1_0
    k = float(len(x_array))
    for i in range(epochs): 
        # Вычисление новых предсказанных значений
        # используется векторизованное умножение и сложение для вектора и константы
        y_pred = b1 * x_array + b0
        # Расчет градиентов
        # np.multiply - поэлементное умножение векторов
        dL_db1 = (-2/k) * np.sum(np.multiply(x_array, (y_array - y_pred)))
        dL_db0 = (-2/k) * np.sum(y_array - y_pred)
        # Изменение значений коэффициентов:
        b1 = b1 - learning_rate * dL_db1
        b0 = b0 - learning_rate * dL_db0
    # Результирующие значения
    y_pred = b1 * x_array + b0
    return b0, b1, y_pred

def show_gradient_descent(epochs, b0_0, b1_0):
    grad_b0, grad_b1, grad_y_pred = gradient_descent(x_array, y_array, b0_0, b1_0, epochs)
    print('b0 = {} - (теоретический), {} - (градиентный спуск)'.format(b0, grad_b0))
    print('b1 = {} - (теоретический), {} - (градиентный спуск)'.format(b1, grad_b1))
    print('MSE = {}'.format(mean_squared_error(y_array_regr, grad_y_pred)))
    plt.plot(x_array, y_array, 'g.')
    plt.plot(x_array, y_array_regr, 'b', linewidth=2.0)
    plt.plot(x_array, grad_y_pred, 'r', linewidth=2.0)
    plt.show()

show_gradient_descent(1000, 150, 900)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# show_gradient_descent(50000, 0, 0)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# show_gradient_descent(100000, 0, 0)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# show_gradient_descent(1000000, 0, 0)

# Обучим линейную регрессию и сравним коэффициенты с рассчитанными ранее
reg1 = LinearRegression().fit(x_array.reshape(-1, 1), y_array.reshape(-1, 1))
(b1, reg1.coef_), (b0, reg1.intercept_)

fig, ax = plt.subplots(figsize=(8,8)) 
sns.scatterplot(ax=ax, x='bmi', y='target', data=df_diabet)

def plot_regr(clf):
    title = clf.__repr__
    clf.fit(x_array.reshape(-1, 1), y_array)
    diabet_y_pred = clf.predict(x_array.reshape(-1, 1))
    fig, ax = plt.subplots(figsize=(5,5))
    ax.set_title(title)
    ax.plot(x_array, y_array, 'b.')
    ax.plot(x_array, diabet_y_pred, 'ro')
    plt.show()

plot_regr(LinearSVR(C=1.0, max_iter=1000))

plot_regr(LinearSVR(C=1.0, loss='squared_epsilon_insensitive', max_iter=1000))

plot_regr(SVR(kernel='poly', degree=5, C=1.0))

plot_regr(SVR(kernel='rbf',  C=1.0))

plot_regr(SVR(kernel='poly', degree=3, C=1.0))

# Обучим дерево на всех признаках
diabet_tree_regr = DecisionTreeRegressor(random_state=1)
diabet_tree_regr.fit(diabet_X_train, diabet_y_train)
diabet_tree_regr

# Визуализация дерева
def get_png_tree(tree_model_param, feature_names_param):
    dot_data = StringIO()
    export_graphviz(tree_model_param, out_file=dot_data, feature_names=feature_names_param,
                    filled=True, rounded=True, special_characters=True)
    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
    return graph.create_png()

# Commented out IPython magic to ensure Python compatibility.
from sklearn.externals.six import StringIO 
from IPython.display import Image
import graphviz 
import pydotplus
import numpy as np
import pandas as pd
from typing import Dict, Tuple
from sklearn.datasets import load_iris, load_wine, load_boston
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
# %matplotlib inline

list1 = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']

from IPython.display import Image
Image(get_png_tree(diabet_tree_regr, list1), height="500")

# Важность признаков
list(zip(diabet_X_train.columns.values, diabet_tree_regr.feature_importances_))

# Важность признаков в сумме дает единицу
sum(diabet_tree_regr.feature_importances_)

from operator import itemgetter

def draw_feature_importances(tree_model, X_dataset, figsize=(15,7)):
    """
    Вывод важности признаков в виде графика
    """
    # Сортировка значений важности признаков по убыванию
    list_to_sort = list(zip(X_dataset.columns.values, tree_model.feature_importances_))
    sorted_list = sorted(list_to_sort, key=itemgetter(1), reverse = True)
    # Названия признаков
    labels = [x for x,_ in sorted_list]
    # Важности признаков
    data = [x for _,x in sorted_list]
    # Вывод графика
    fig, ax = plt.subplots(figsize=figsize)
    ind = np.arange(len(labels))
    plt.bar(ind, data)
    plt.xticks(ind, labels, rotation='vertical')
    # Вывод значений
    for a,b in zip(ind, data):
        plt.text(a-0.05, b+0.01, str(round(b,3)))
    plt.show()
    return labels, data

diabet_tree_regr_fl, diabet_tree_regr_fd = draw_feature_importances(diabet_tree_regr, diabet_X_train)

# Список признаков, отсортированный на основе важности, и значения важности
diabet_tree_regr_fl, diabet_tree_regr_fd

diabet_X_train.head()

# Пересортируем признаки на основе важности
X_train_sorted = diabet_X_train[diabet_tree_regr_fl]
X_train_sorted.head()

Y_predict_1 = diabet_tree_regr.predict(diabet_X_test)

mean_absolute_error(diabet_y_test, Y_predict_1)

# Обучим дерево и предскажем результаты на пяти лучших признаках 
diabet_tree_regr_2= DecisionTreeRegressor(random_state=1).fit(
    X_train_sorted[diabet_tree_regr_fl[0:5]], diabet_y_train)
Y_predict_2 = diabet_tree_regr_2.predict(diabet_X_test[diabet_tree_regr_fl[0:5]])

# Ошибка стала меньше
mean_absolute_error(diabet_y_test, Y_predict_2)

# Исследуем, как изменяется ошибка при добавлении признаков в порядке значимости
diabet_X_range = list(range(1, len(diabet_X_train.columns)+1))
diabet_X_range

mae_list = []
for i in diabet_X_range:
    # Обучим дерево и предскажем результаты на заданном количестве признаков 
    diabet_tree_regr_3 = DecisionTreeRegressor(random_state=1).fit(
        diabet_X_train[diabet_tree_regr_fl[0:i]], diabet_y_train)
    Y_predict_3 = diabet_tree_regr_3.predict(diabet_X_test[diabet_tree_regr_fl[0:i]])
    temp_mae = mean_absolute_error(diabet_y_test, Y_predict_3)
    mae_list.append(temp_mae)

plt.subplots(figsize=(10,5))
plt.plot(diabet_X_range, mae_list)
for a,b in zip(diabet_X_range, mae_list):
    plt.text(a-0.05, b+0.01, str(round(b,3)))
plt.show()

pred = reg1.predict(x_array.reshape(-1,1))
print("r2_score: ", r2_score(y_array, pred))
print("mean_squared_error", mean_squared_error(y_array, pred))

svr = SVR(kernel='rbf')
svr.fit(diabet_X_train, diabet_y_train)
print("r2_score: ", r2_score(diabet_y_test,svr.predict(diabet_X_test)))
print("mean_squared_error", mean_squared_error(diabet_y_test,svr.predict(diabet_X_test)))

print("r2_score: ", r2_score(diabet_y_test, diabet_tree_regr.predict(diabet_X_test)))
print("mean_squared_error", mean_squared_error(diabet_y_test, diabet_tree_regr.predict(diabet_X_test)))